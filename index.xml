<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>0x74696d</title>
    <link>http://0x74696d.com/index.xml</link>
    <description>Recent content on 0x74696d</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Feb 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://0x74696d.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Be Careful What You Benchmark</title>
      <link>http://0x74696d.com/posts/be-careful-what-you-benchmark/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/be-careful-what-you-benchmark/</guid>
      <description>

&lt;p&gt;I have a project where I need to generate unique database keys at the client rather than using autoincrementing keys, in order to support sharding at the client. (I&#39;m effectively re-implementing &lt;a href=&#34;https://codeascraft.com/2012/04/20/two-sides-for-salvation/&#34;&gt;Etsy&#39;s MySQL master-master scheme&lt;/a&gt; if you want to know more.) I don&#39;t want to use UUIDs because they&#39;re 128 bits instead of 64 bits, and because they&#39;re not time ordered they can result in &lt;a href=&#34;https://cjsavage.com/guides/mysql/insert-perf-uuid-vs-ordered-uuid-vs-int-pk.html&#34;&gt;poor insert performance&lt;/a&gt;. So I&#39;m implementing Twitter&#39;s now-retired &lt;a href=&#34;https://github.com/twitter/snowflake/tree/snowflake-2010&#34;&gt;Snowflake&lt;/a&gt;, which produces roughly-ordered but unique IDs. In so doing, I ran into an interesting performance riddle that&#39;s worth sharing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Be forewarned this whole post is a bit of a shaggy dog story. There isn&#39;t a satisfying conclusion with a smoking gun at the end, just a place to start work and some lessons about being careful around your assumptions about your test environment. But if you&#39;re interested in benchmarking golang code and how virtualization can foul the results, you might find it worth your while.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;making-snowflakes&#34;&gt;Making Snowflakes&lt;/h2&gt;

&lt;p&gt;The Snowflakes I&#39;m generating are 64 bit unsigned ints. The original Twitter Snowflake used int64, or 63 useful bits plus the unused sign bit, for what I assume have to do with the same legacy reasons they cited for wanting 64 bits in the first place. The first 41 bits are milliseconds since a custom epoch, which gives me 69 years worth of IDs. The major constraint here is that we have to ensure we&#39;re using a clock that never goes backwards within the lifetime of a process. The next 11 bits are a unique worker ID, which gives me 2048 worker processes.&lt;/p&gt;

&lt;p&gt;These worker IDs are the sole point of coordination required; whatever schedules the worker will need to reserve the worker IDs. In practice this is 100 times more workers than I&#39;m likely to need for this project but that gives me plenty of headroom. It&#39;s safe to recycle worker IDs so long as they&#39;re unique among the &lt;em&gt;running&lt;/em&gt; workers. The last 12 bits of the ID are a monotonic sequence ID. Each worker can therefore generate a maximum of 4096 IDs per millisecond.&lt;/p&gt;

&lt;p&gt;That last requirement means that our ID &amp;quot;server&amp;quot; for each worker process needs to make sure it doesn&#39;t issue sequence IDs larger than 4095 within a single millisecond, otherwise we overflow and end up repeating IDs. I also want the server to be safe to use by multiple threads (or goroutines in this case because I&#39;m writing it in golang because... I&#39;m too dumb to use generics?). I poked around on GitHub for existing implementations first without much satisfaction.&lt;/p&gt;

&lt;p&gt;One version was a straight port of Twitter&#39;s Scala code which looked OK but obviously wasn&#39;t very idiomatic golang code. Another was laughably unthreadsafe. Another had an API that tangled up the server setup with implementation details for their worker ID generation. And so on. All the implementations I found return a &lt;code&gt;uint64&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt; directly without wrapping it. I want any consuming code I write later to notice if I accidentally do something stupid like try to do math on an ID, so embedding the &lt;code&gt;uint64&lt;/code&gt; in a &lt;code&gt;Snowflake&lt;/code&gt; type gives me a modicum of type safety. There was also a nagging doubt in my mind about the number of times the original design was checking the time. (&lt;em&gt;Ooo... foreshadowing!&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;So I implemented my own version, where the server listens on a channel for millisecond ticks and resets the sequence ID counter. In theory this would avoid a lot of calls to get the time but would involve an extra mutex per millisecond-long cycle. Because I&#39;m paranoid I wanted to make sure I wasn&#39;t missing some profound performance regression in dumping the original Twitter design.&lt;/p&gt;

&lt;p&gt;I took the library that was the closest direct implementation of Twitter&#39;s design, &lt;a href=&#34;https://github.com/sdming/gosnow&#34;&gt;&lt;code&gt;sdming/gosnow&lt;/code&gt;&lt;/a&gt;, and ported it to use the same API as my own, and also took a slightly different approach, Matt Ho&#39;s &lt;a href=&#34;https://github.com/savaki/snowflake&#34;&gt;&lt;code&gt;savaki/snowflake&lt;/code&gt;&lt;/a&gt;, and gave it the same treatment. After writing a bunch of test code to make sure all three implementations were correct in behavior, I put them all through the same benchmark (each with different names of course):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Snowflake&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// avoids compiler optimizing-away output&lt;/span&gt;

&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;BenchmarkSnowflakeGen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;testing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
	&lt;span class=&#34;nx&#34;&gt;server&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;NewServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
	&lt;span class=&#34;kd&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Snowflake&lt;/span&gt;
	&lt;span class=&#34;nx&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ResetTimer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
	&lt;span class=&#34;nx&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;RunParallel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;pb&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;testing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PB&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
		&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;pb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
			&lt;span class=&#34;nx&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;server&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;GetSnowflake&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
		&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
	&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
	&lt;span class=&#34;nx&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;id&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&#34;the-benchmarks&#34;&gt;The Benchmarks&lt;/h2&gt;

&lt;p&gt;Before I ran the benchmark I wanted to do some reasoning from first principles. Because I can&#39;t generate more than 4096 IDs per millisecond, I know that we have a lower bound of 244 nanoseconds assuming we can saturate the server. A single thread could manage this only if all the rest of the code was free. So that&#39;s our target for multi-threaded code. I ran through all three benchmarks on my Linux development machine and I was pretty happy with the result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /proc/cpuinfo | grep &amp;quot;model name&amp;quot; | head -1
model name      : Intel(R) Core(TM) i7-4712HQ CPU @ 2.30GHz

$ nproc
8

$ go test -v -run NULL -benchmem -bench .
BenchmarkSavakiSnowflakeGen-8    5000000     334 ns/op    0 B/op   0 allocs/op
BenchmarkSnowflakeGen-8          5000000     244 ns/op    0 B/op   0 allocs/op
BenchmarkTwitterSnowflakeGen-8   5000000     292 ns/op    0 B/op   0 allocs/op
PASS
ok      snowflake    5.247s

$ GOMAXPROCS=4 go test -v -run NULL -benchmem -bench .
BenchmarkSavakiSnowflakeGen-4    5000000     360 ns/op    0 B/op   0 allocs/op
BenchmarkSnowflakeGen-4          5000000     244 ns/op    0 B/op   0 allocs/op
BenchmarkTwitterSnowflakeGen-4   5000000     250 ns/op    0 B/op   0 allocs/op
PASS
ok      snowflake    5.148s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My own implementation hits the lower-bound benchmark! But the Savaki and Twitter clone library both do respectably, so my concerns about lots of calls to &lt;code&gt;gettimeofday&lt;/code&gt; seem to have been for nothing. Note that I&#39;ve run this on both 8 cores and 4 (via &lt;code&gt;GOMAXPROCS&lt;/code&gt;) to check for hidden &lt;a href=&#34;https://en.wikipedia.org/wiki/Amdahl&#39;s_law&#34;&gt;Amdahl&#39;s Law&lt;/a&gt; problems. This and all other tests are running under golang 1.7.&lt;/p&gt;

&lt;p&gt;All done, right? Well I&#39;m not going to be deploying on my 8-core Linux laptop. I&#39;ll probably be deploying to a Docker container on &lt;a href=&#34;https://www.joyent.com/triton&#34;&gt;Triton&lt;/a&gt;, so I wanted to make sure the relative benchmark would hold up there. I didn&#39;t happen to have Triton keys on my Linux laptop so I switched over to my Mac that did. But first I needed to build a container with the code so I did that and sanity-checked it under Docker for Mac. At this point someone in the audience is slapping their head and saying &amp;quot;duh!&amp;quot;, to which I can only say &amp;quot;ssh... you&#39;ll spoil it for everyone else!&amp;quot;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nproc
4

$ go test -v -run NULL -benchmem -bench .
BenchmarkSavakiSnowflakeGen-4     500000    3228 ns/op    0 B/op   0 allocs/op
BenchmarkSnowflakeGen-4          5000000     273 ns/op    0 B/op   0 allocs/op
BenchmarkTwitterSnowflakeGen-4   1000000    1086 ns/op    0 B/op   0 allocs/op
PASS
ok      _/go/snowflake       4.430s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Wat?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here&#39;s where we go off the rails a bit. The first thing I did, which wasn&#39;t all that necessary, was to run the benchmark in other environments to make sure I hadn&#39;t made some strange mistake.  There&#39;s no allocation in any of the algorithms, so from here on out I&#39;ll elide the &lt;code&gt;-benchmem&lt;/code&gt; data.&lt;/p&gt;

&lt;p&gt;Here I run it on the MacOS host:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sysctl -n hw.ncpu
8

$ sysctl -n machdep.cpu.brand_string
Intel(R) Core(TM) i7-4750HQ CPU @ 2.00GHz

$ go test -v -run NULL -bench .
BenchmarkSavakiSnowflakeGen-8       5000000      291 ns/op
BenchmarkSnowflakeGen-8             5000000      256 ns/op
BenchmarkTwitterSnowflakeGen-8      5000000      291 ns/op
PASS
ok      snowflake    5.045s

$ GOMAXPROCS=4 go test -v -run NULL -bench .
BenchmarkSavakiSnowflakeGen-4       5000000      342 ns/op
BenchmarkSnowflakeGen-4             5000000      245 ns/op
BenchmarkTwitterSnowflakeGen-4      5000000      244 ns/op
PASS
ok      snowflake    4.989s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here I run it on a &lt;code&gt;c4.xlarge&lt;/code&gt; on AWS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nproc
4

$ cat /proc/cpuinfo | grep &amp;quot;model name&amp;quot; | head -1
model name      : Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz

$ go test -v -run NULL -bench .
BenchmarkSavakiSnowflakeGen-4       5000000      373 ns/op
BenchmarkSnowflakeGen-4             5000000      263 ns/op
BenchmarkTwitterSnowflakeGen-4     10000000      247 ns/op
PASS
ok      _/go/snowflake       6.228s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here I run it on a Triton &lt;code&gt;g4-highcpu-4G&lt;/code&gt;. Note that in this case we&#39;re running on a bare metal container where we can see all cores of the underlying host but the process is only scheduled time across those processors according to the &lt;a href=&#34;https://wiki.smartos.org/display/DOC/Managing+CPU+Cycles+in+a+Zone&#34;&gt;Fair Share Scheduler (FSS)&lt;/a&gt;. This instance has CPU shares very roughly equivalent to the same 4 vCPU that the AWS instance has, albeit on slightly slower processors and at half the price. This benchmark is also &lt;em&gt;pure&lt;/em&gt; CPU which means that Triton&#39;s profound I/O advantages in avoiding hardware virtualization don&#39;t come into play at all! So don&#39;t try to directly compare the &amp;quot;marketing number&amp;quot; of 4 vCPU.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nproc
48
$ cat /proc/cpuinfo | grep &amp;quot;model name&amp;quot; | head -1
model name      : Intel(r) Xeon(r) CPU E5-2690 v3 @ 2.60GHz

# wow, lots of Amdahl&#39;s law appears with 48 processors visible!
$ go test -v -run NULL -bench .
BenchmarkSavakiSnowflakeGen-48      1000000     1280 ns/op
BenchmarkSnowflakeGen-48            1000000     1135 ns/op
BenchmarkTwitterSnowflakeGen-48     1000000     1414 ns/op
PASS
ok      _/go/snowflake       4.778s

$ GOMAXPROCS=8 go test -v -run NULL -bench .
BenchmarkSavakiSnowflakeGen-8       3000000      523 ns/op
BenchmarkSnowflakeGen-8             5000000      437 ns/op
BenchmarkTwitterSnowflakeGen-8      2000000      659 ns/op
PASS
ok      _/go/snowflake       6.721s

$ GOMAXPROCS=4 go test -v -run NULL  -bench .
BenchmarkSavakiSnowflakeGen-4       3000000      449 ns/op
BenchmarkSnowflakeGen-4             5000000      356 ns/op
BenchmarkTwitterSnowflakeGen-4      3000000      497 ns/op
PASS
ok      _/go/snowflake       6.001s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so now that we&#39;ve run the benchmark everywhere we see that the problem appears to be unique to Docker for Mac. Which isn&#39;t a production environment so no need to get bent out of shape about that. Right? Well, you&#39;ll note from the position of your scroll bar that perhaps I decided to get bent out of shape about it anyways, just out of morbid curiosity.&lt;/p&gt;

&lt;h2 id=&#34;profiling&#34;&gt;Profiling&lt;/h2&gt;

&lt;p&gt;The next step was to take a profiler to both implementations on both my development machine and in Docker for Mac. It would have been neat to see the profiling on MacOS as well by way of comparison but apparently go&#39;s profiling is totally broken outside of Linux.&lt;/p&gt;

&lt;p&gt;First I ran the benchmark again on my Linux laptop, this time outputting a CPU profile. I&#39;ll then run this output through go&#39;s &lt;code&gt;pprof&lt;/code&gt; tool. I first looked at the top 20 calls, sorted by cumulative time. The cumulative time in &lt;code&gt;pprof&lt;/code&gt; refers to the total amount of time spent in the call including all its callees. As we&#39;d hope and expect for a tight-looping benchmark, we spend the vast majority of our time in the code we&#39;re benchmarking. Perhaps unsurprisingly we spend about 70% of our time locking and unlocking a mutex.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -v -run NULL -bench BenchmarkSnowflake -cpuprofile cpu-snowflake.prof
BenchmarkSnowflakeGen-8             5000000      244 ns/op
PASS
ok      snowflake 1.471s

$ go tool pprof cpu-snowflake.prof
Entering interactive mode (type &amp;quot;help&amp;quot; for commands)
(pprof) top20 -cum
2.77s of 3.65s total (75.89%)
Dropped 8 nodes (cum &amp;lt;= 0.02s)
Showing top 20 nodes out of 46 (cum &amp;gt;= 0.21s)
      flat  flat%   sum%        cum   cum%
     0.33s  9.04%  9.04%      2.88s 78.90%  snowflake.(*Server).GetSnowflake
         0     0%  9.04%      2.87s 78.63%  runtime.goexit
     0.01s  0.27%  9.32%      2.86s 78.36%  snowflake.BenchmarkSnowflakeGen.func1
         0     0%  9.32%      2.86s 78.36%  testing.(*B).RunParallel.func1
     0.51s 13.97% 23.29%      1.88s 51.51%  sync.(*Mutex).Lock
     0.83s 22.74% 46.03%      0.83s 22.74%  sync/atomic.CompareAndSwapUint32
     0.01s  0.27% 46.30%      0.74s 20.27%  runtime.mcall
         0     0% 46.30%      0.73s 20.00%  runtime.park_m
         0     0% 46.30%      0.68s 18.63%  runtime.schedule
     0.10s  2.74% 49.04%      0.67s 18.36%  sync.(*Mutex).Unlock
     0.02s  0.55% 49.59%      0.65s 17.81%  runtime.findrunnable
     0.06s  1.64% 51.23%      0.38s 10.41%  runtime.lock
     0.01s  0.27% 51.51%      0.34s  9.32%  sync.runtime_Semacquire
     0.03s  0.82% 52.33%      0.33s  9.04%  runtime.semacquire
     0.31s  8.49% 60.82%      0.31s  8.49%  runtime/internal/atomic.Xchg
     0.27s  7.40% 68.22%      0.27s  7.40%  sync/atomic.AddUint32
     0.02s  0.55% 68.77%      0.25s  6.85%  runtime.semrelease
         0     0% 68.77%      0.25s  6.85%  sync.runtime_Semrelease
     0.05s  1.37% 70.14%      0.21s  5.75%  runtime.unlock
     0.21s  5.75% 75.89%      0.21s  5.75%  runtime/internal/atomic.Xadd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But where is that mutex? For clarity we can list the &lt;code&gt;GetSnowflake&lt;/code&gt; function and see the call times compared to each section of code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(pprof)  list GetSnowflake
Total: 3.65s
ROUTINE ======================== snowflake.(*Server).GetSnowflake in snowflake.go
     330ms      2.94s (flat, cum) 80.55% of Total
         .          .     87:
         .          .     88:// GetSnowflake ...
         .          .     89:func (s *Server) GetSnowflake() Snowflake {
         .          .     90:   // we don&#39;t exit scope except in the happy case where we
         .          .     91:   // return, so we can&#39;t &#39;defer s.lock.Unlock()&#39; here
      30ms      1.91s     92:   s.lock.Lock()
     140ms      140ms     93:   sequence := s.id
         .          .     94:   if sequence &amp;gt; maxSequence {
         .          .     95:           s.lock.Unlock()
         .       60ms     96:           return s.GetSnowflake()
         .          .     97:   }
         .          .     98:   s.id++
      10ms       10ms     99:   timestamp := s.lastTimestamp
      10ms      680ms    100:   s.lock.Unlock()
         .          .    101:   return Snowflake(
     140ms      140ms    102:           (timestamp &amp;lt;&amp;lt; (workerIDBits + sequenceBits)) |
         .          .    103:                   (uint64(s.workerID) &amp;lt;&amp;lt; sequenceBits) |
         .          .    104:                   (uint64(sequence)))
         .          .    105:}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code perhaps makes little sense without the rest of the server code. Here&#39;s that.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// Server ...&lt;/span&gt;
&lt;span class=&#34;kd&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Server&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;workerID&lt;/span&gt;      &lt;span class=&#34;nx&#34;&gt;workerID&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;ticker&lt;/span&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Ticker&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;lock&lt;/span&gt;          &lt;span class=&#34;nx&#34;&gt;sync&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Mutex&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;lastTimestamp&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;uint64&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;id&lt;/span&gt;            &lt;span class=&#34;kt&#34;&gt;uint16&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// NewServer ...&lt;/span&gt;
&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;NewServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Server&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;ticker&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;NewTicker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Millisecond&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;server&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Server&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;        &lt;span class=&#34;nx&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;workerID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;      &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;lock&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;          &lt;span class=&#34;nx&#34;&gt;sync&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Mutex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;            &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;lastTimestamp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;uint64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;UnixNano&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;nano&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;idEpoch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;go&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tick&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;range&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;C&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                        &lt;span class=&#34;nx&#34;&gt;server&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;reset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}()&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;server&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Server&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;reset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;tick&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;lock&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Lock&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;defer&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;lock&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Unlock&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;lastTimestamp&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;uint64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;UnixNano&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;nano&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;idEpoch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let&#39;s now do the same for the Twitter port. A bit less time being spent in the mutex and a bit more spent switching in the runtime. Nothing unexpected. I&#39;ve provided the code listing below again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go tool pprof cpu-snowflake-twitter.prof
Entering interactive mode (type &amp;quot;help&amp;quot; for commands)
(pprof) top20 -cum
2.50s of 4s total (62.50%)
Dropped 16 nodes (cum &amp;lt;= 0.02s)
Showing top 20 nodes out of 61 (cum &amp;gt;= 0.25s)
      flat  flat%   sum%        cum   cum%
     0.03s  0.75%  0.75%      2.94s 73.50%  snowflake.BenchmarkTwitterSnowflakeGen.func1
         0     0%  0.75%      2.94s 73.50%  runtime.goexit
         0     0%  0.75%      2.94s 73.50%  testing.(*B).RunParallel.func1
     0.24s  6.00%  6.75%      2.88s 72.00%  snowflake.(*TwitterServer).GetSnowflake
     0.29s  7.25% 14.00%      1.46s 36.50%  sync.(*Mutex).Lock
     0.03s  0.75% 14.75%      0.87s 21.75%  runtime.mcall
     0.03s  0.75% 15.50%      0.84s 21.00%  runtime.park_m
     0.02s   0.5% 16.00%      0.77s 19.25%  runtime.schedule
     0.04s  1.00% 17.00%      0.75s 18.75%  sync.(*Mutex).Unlock
     0.18s  4.50% 21.50%      0.73s 18.25%  runtime.lock
     0.12s  3.00% 24.50%      0.70s 17.50%  runtime.findrunnable
     0.08s  2.00% 26.50%      0.57s 14.25%  runtime.semacquire
         0     0% 26.50%      0.57s 14.25%  sync.runtime_Semacquire
     0.01s  0.25% 26.75%      0.54s 13.50%  sync.runtime_Semrelease
     0.06s  1.50% 28.25%      0.53s 13.25%  runtime.semrelease
     0.11s  2.75% 31.00%      0.44s 11.00%  runtime.systemstack
     0.42s 10.50% 41.50%      0.42s 10.50%  sync/atomic.CompareAndSwapUint32
     0.33s  8.25% 49.75%      0.33s  8.25%  runtime.procyield
     0.26s  6.50% 56.25%      0.26s  6.50%  runtime/internal/atomic.Xchg
     0.25s  6.25% 62.50%      0.25s  6.25%  runtime/internal/atomic.Cas

(pprof) list GetSnowflake
Total: 4s
ROUTINE ======================== snowflake.(*TwitterServer).GetSnowflake in snowflake/snowflake_twitter.go
     240ms      2.88s (flat, cum) 72.00% of Total
         .          .     41:
         .          .     42:// GetSnowflake ...
         .          .     43:func (s *TwitterServer) GetSnowflake() Snowflake {
         .          .     44:
      20ms       40ms     45:   ts := uint64(time.Now().UnixNano()/nano - idTwitterEpoch)
     120ms      1.58s     46:   s.lock.Lock()
      20ms      220ms     47:   defer s.lock.Unlock()
         .          .     48:   // TODO: need to block on clock running backwards
         .          .     49:
      40ms       40ms     50:   if s.lastTimestamp == ts {
      10ms       10ms     51:           s.id = (s.id + 1) &amp;amp; maxSequence
         .          .     52:           if s.id == 0 {
         .          .     53:                   for ts &amp;lt;= s.lastTimestamp {
         .          .     54:                           time.Sleep(10 * time.Microsecond)
         .          .     55:                           ts = uint64(time.Now().UnixNano()/nano - idTwitterEpoch)
         .          .     56:                   }
         .          .     57:           }
         .          .     58:   } else {
         .          .     59:           s.id = 0
         .          .     60:   }
         .          .     61:   s.lastTimestamp = ts
         .          .     62:   id := Snowflake(
      20ms       20ms     63:           (s.lastTimestamp &amp;lt;&amp;lt; (workerIDBits + sequenceBits)) |
         .          .     64:                   (uint64(s.workerID) &amp;lt;&amp;lt; sequenceBits) |
      10ms       10ms     65:                   (uint64(s.id)))
         .      960ms     66:   return id
         .          .     67:
         .          .     68:}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&#39;ve replicated the results above under hardware virtualized Linux (the AWS &lt;code&gt;c4.xlarge&lt;/code&gt;) and bare metal containers (the Triton &lt;code&gt;g4-highcpu-4G&lt;/code&gt;) without learning anything new. From this point in the interest of space I&#39;m going to abandon analysis of the &lt;code&gt;SavakiServer&lt;/code&gt; and focus on just my version and the Twitter version. You&#39;ll have to trust me when I say that it matches the Twitter version pretty closely except in some small details. But now I&#39;ll take the cpu profiling to the Docker for Mac environment.&lt;/p&gt;

&lt;p&gt;First my Snowflake server, which is mostly unchanged as we&#39;d expect from the original benchmark values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go tool pprof cpu-snowflake.prof
Entering interactive mode (type &amp;quot;help&amp;quot; for commands)
(pprof) top20 -cum
1.51s of 1.85s total (81.62%)
Showing top 20 nodes out of 58 (cum &amp;gt;= 0.04s)
      flat  flat%   sum%        cum   cum%
     0.19s 10.27% 10.27%      1.64s 88.65%  _/go/snowflake.(*Server).GetSnowflake
         0     0% 10.27%      1.39s 75.14%  runtime.goexit
     0.03s  1.62% 11.89%      1.37s 74.05%  _/go/snowflake.BenchmarkSnowflakeGen.func1
         0     0% 11.89%      1.37s 74.05%  testing.(*B).RunParallel.func1
     0.32s 17.30% 29.19%      1.08s 58.38%  sync.(*Mutex).Lock
     0.49s 26.49% 55.68%      0.49s 26.49%  sync/atomic.CompareAndSwapUint32
     0.12s  6.49% 62.16%      0.37s 20.00%  sync.(*Mutex).Unlock
     0.19s 10.27% 72.43%      0.19s 10.27%  sync/atomic.AddUint32
     0.16s  8.65% 81.08%      0.16s  8.65%  runtime.procyield
     0.01s  0.54% 81.62%      0.16s  8.65%  sync.runtime_doSpin
         0     0% 81.62%      0.08s  4.32%  runtime.mcall
         0     0% 81.62%      0.08s  4.32%  runtime.park_m
         0     0% 81.62%      0.08s  4.32%  runtime.semacquire
         0     0% 81.62%      0.08s  4.32%  sync.runtime_Semacquire
         0     0% 81.62%      0.06s  3.24%  runtime.findrunnable
         0     0% 81.62%      0.06s  3.24%  runtime.schedule
         0     0% 81.62%      0.05s  2.70%  runtime.copystack
         0     0% 81.62%      0.05s  2.70%  runtime.morestack
         0     0% 81.62%      0.05s  2.70%  runtime.newstack
         0     0% 81.62%      0.04s  2.16%  runtime.cansemacquire

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now for the Twitter server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go tool pprof cpu-snowflake-twitter.prof
Entering interactive mode (type &amp;quot;help&amp;quot; for commands)
(pprof) top20 -cum
3.12s of 3.22s total (96.89%)
Dropped 9 nodes (cum &amp;lt;= 0.02s)
Showing top 20 nodes out of 24 (cum &amp;gt;= 0.02s)
      flat  flat%   sum%        cum   cum%
         0     0%     0%      2.84s 88.20%  runtime._System
     2.83s 87.89% 87.89%      2.83s 87.89%  runtime._ExternalCode
     0.01s  0.31% 88.20%      0.35s 10.87%  _/go/snowflake.BenchmarkTwitterSnowflakeGen.func1
         0     0% 88.20%      0.35s 10.87%  runtime.goexit
         0     0% 88.20%      0.35s 10.87%  testing.(*B).RunParallel.func1
     0.02s  0.62% 88.82%      0.34s 10.56%  _/go/snowflake.(*TwitterServer).GetSnowflake
     0.07s  2.17% 90.99%      0.27s  8.39%  sync.(*Mutex).Lock
     0.10s  3.11% 94.10%      0.10s  3.11%  runtime.procyield
         0     0% 94.10%      0.10s  3.11%  sync.runtime_doSpin
     0.02s  0.62% 94.72%      0.04s  1.24%  runtime.deferreturn
     0.04s  1.24% 95.96%      0.04s  1.24%  sync/atomic.CompareAndSwapUint32
         0     0% 95.96%      0.03s  0.93%  runtime.mcall
         0     0% 95.96%      0.03s  0.93%  runtime.park_m
     0.01s  0.31% 96.27%      0.03s  0.93%  runtime.schedule
         0     0% 96.27%      0.03s  0.93%  runtime.semacquire
         0     0% 96.27%      0.03s  0.93%  sync.runtime_Semacquire
     0.01s  0.31% 96.58%      0.03s  0.93%  sync.runtime_canSpin
         0     0% 96.58%      0.02s  0.62%  runtime.findrunnable
     0.01s  0.31% 96.89%      0.02s  0.62%  runtime.runqempty
         0     0% 96.89%      0.02s  0.62%  runtime.runqgrab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally a clue! But what is &lt;code&gt;runtime._System&lt;/code&gt; and &lt;code&gt;runtime._ExternalCode&lt;/code&gt;? Well the golang docs don&#39;t give us much help here but this post by &lt;a href=&#34;https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs&#34;&gt;Dmitry Vyukov on a blog for Intel&lt;/a&gt; has this to say:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;System means time spent in goroutine scheduler, stack management code and other auxiliary runtime code. ExternalCode means time spent in native dynamic libraries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We&#39;re pushing into areas where I don&#39;t have deep expertise, but given that I&#39;ve got a statically linked binary here, this sounds like we&#39;re talking about system calls. So I put &lt;code&gt;strace&lt;/code&gt; on the benchmark runs and counted up the calls. I&#39;ve elided all the noise here and taken the top handful of syscalls from each one.&lt;/p&gt;

&lt;p&gt;My version, on native Linux.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ strace -cf go test -run NULL -bench BenchmarkSnowflake

% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 74.57    1.592028          42     37914      8385 futex
 25.40    0.542223           9     62410           select
  0.02    0.000493           0     19141           sched_yield
  0.00    0.000053           5        10           wait4
  0.00    0.000015           0      1766           read
  0.00    0.000015           0        80        55 unlinkat
  ...
------ ----------- ----------- --------- --------- ----------------
100.00    2.134836                128426      8858 total
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Twitter version, on native Linux.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$  strace -cf go test -run NULL -bench BenchmarkTwitter
...
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 78.46    1.642500          35     46413      5088 futex
 20.76    0.434634           9     51023           select
  0.77    0.016064        1606        10           wait4
  0.01    0.000137           0     20525           sched_yield
  0.00    0.000022           0       833           close
  0.00    0.000017           0      1528        13 lstat
  0.00    0.000012           0       604           rt_sigaction
  0.00    0.000012           2         6           rt_sigreturn
  ...
------ ----------- ----------- --------- --------- ----------------
100.00    2.093406                126847      5561 total
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My version, on Docker for Mac.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ strace -cf go test -run NULL -bench BenchmarkSnowflake
...
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 76.19    4.201271         550      7641      1664 futex
 19.11    1.053696         119      8856           select
  3.27    0.180172         101      1776           read
  0.52    0.028537           2     16723           clock_gettime
  0.37    0.020437        2044        10           wait4
  0.36    0.020000        3333         6           waitid
  0.17    0.009448          12       785           sched_yield
  0.01    0.000342          57         6           rt_sigreturn
  ...
------ ----------- ----------- --------- --------- ----------------
100.00    5.514387                 42723      2108 total
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Twitter version, on Docker for Mac.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ strace -cf go test -run NULL -bench BenchmarkTwitter
...
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 67.48    2.568430        1468      1750       258 futex
 21.61    0.822401         707      1164           select
 10.63    0.404649           3    118067           clock_gettime
  0.27    0.010230        1023        10           wait4
  0.00    0.000171           0      1515         9 lstat
  0.00    0.000136           0       749           openat
  ...
------ ----------- ----------- --------- --------- ----------------
100.00    3.806243                129994       702 total
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We expect to see lots of &lt;code&gt;futex&lt;/code&gt; calls (&amp;quot;fast user space mutex&amp;quot;) because the golang runtime uses it a bunch under the hood for scheduling and our &lt;code&gt;mutex.Lock&lt;/code&gt;/&lt;code&gt;mutex.ULock&lt;/code&gt; calls. We can see that my version spends more of its total time in system calls than the Twitter version in both cases. We can also see the large amount of overhead that &lt;code&gt;strace&lt;/code&gt; is incurring; these benchmarks only take ~1sec without it! If only Linux had dtrace! So we should take the timing with a heaping tablespoon of salt. But now wait a second, where is &lt;code&gt;clock_gettime&lt;/code&gt; in the native Linux version?&lt;/p&gt;

&lt;h2 id=&#34;you-know-what-time-it-is&#34;&gt;You know what time it is&lt;/h2&gt;

&lt;p&gt;It&#39;s time to get &lt;a href=&#34;http://man7.org/linux/man-pages/man7/vdso.7.html&#34;&gt;vDSO&lt;/a&gt;! (That probably sounded cooler in my head.) Virtual dynamic shared objects are a pretty nifty performance optimization. Normally when a userspace program makes a system call, there&#39;s a fairly expensive bit of shuffling around of memory where some registers are set, kernel space memory gets swapped in, more registers are set, and kernel memory gets swapped back out. But there are lots of programs that want to make lots and lots of system calls like checking what time it is. So vDSOs map a read-only page of kernel memory associated with certain system call functions into userspace. This way userspace applications can use those functions without going through the context switch of a syscall. I&#39;m not super-familiar with the history of this feature but there&#39;s an older mechanism called vsyscall that vDSO has replaced, and it looks like the feature started in ever-practical-but-often-a-bit-gross Linux and got ported or emulated in the BSDs and Illumos family later down the road. (I&#39;m relying on &lt;a href=&#34;https://meta.wikimedia.org/wiki/Cunningham&#39;s_Law&#34;&gt;Cunningham&#39;s Law&lt;/a&gt; to correct me on this point.)&lt;/p&gt;

&lt;p&gt;We can use &lt;code&gt;go tool pprof&lt;/code&gt; to disassemble our function but we don&#39;t get real assembler in this case. Instead we get a kind of &lt;a href=&#34;http://dtrace.org/blogs/wesolows/2014/12/29/golang-is-trash/&#34;&gt;pseudo-assembly&lt;/a&gt; that&#39;s specific to the golang runtime.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# irrelevant sections elided
(pprof) disasm GetSnowflake
Total: 4.15s
ROUTINE ======================== snowflake.(*TwitterServer).GetSnowflake
     230ms      3.17s (flat, cum) 76.39% of Total
         .       20ms     46ee6a: CALL time.Now(SB)
         .          .     46ef47: CALL time.Now(SB)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But we can take the test binary built by the CPU profiler and dump it to see how we&#39;re calling system calls there to see if vDSO might be a factor here.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# machine code bytes elided for space
$  objdump -d snowflake.test| grep gettime
45887e:   mov    0x16935b(%rip),%rax  # 5c1be0 &amp;lt;runtime.__vdso_clock_gettime_sym&amp;gt;
4588b5:   mov    0x139864(%rip),%rax  # 592120 &amp;lt;runtime.__vdso_gettimeofday_sym&amp;gt;
4588ee:   mov    0x1692eb(%rip),%rax  # 5c1be0 &amp;lt;runtime.__vdso_clock_gettime_sym&amp;gt;
45892e:   mov    0x1397eb(%rip),%rax  # 592120 &amp;lt;runtime.__vdso_gettimeofday_sym&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A bit of googling leads us to &lt;a href=&#34;https://golang.org/src/runtime/sys_linux_amd64.s#L138&#34;&gt;this bit of golang assembly&lt;/a&gt; in the golang runtime code. Note that this is &lt;em&gt;not&lt;/em&gt; x86 assembly, and even it were I&#39;m running into the limits of my useful knowledge. Presumably it&#39;s calling into the vDSO page that&#39;s being exposed to userspace.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// func now() (sec int64, nsec int32)
TEXT time.now(SB),NOSPLIT,$16
    // Be careful. We&#39;re calling a function with gcc calling convention here.
    // We&#39;re guaranteed 128 bytes on entry, and we&#39;ve taken 16, and the
    // call uses another 8.
    // That leaves 104 for the gettime code to use. Hope that&#39;s enough!
    MOVQ  runtime.__vdso_clock_gettime_sym(SB), AX
    CMPQ  AX, $0
    JEQ   fallback
    MOVL  $0, DI // CLOCK_REALTIME
    LEAQ  0(SP), SI
    CALL  AX
    MOVQ  0(SP), AX// sec
    MOVQ  8(SP), DX// nsec
    MOVQ  AX, sec+0(FP)
    MOVL  DX, nsec+8(FP)
    RET
fallback:
    LEAQ  0(SP), DI
    MOVQ  $0, SI
    MOVQ  runtime.__vdso_gettimeofday_sym(SB), AX
    CALL  AX
    MOVQ  0(SP), AX// sec
    MOVL  8(SP), DX// usec
    IMUL  Q$1000, DX
    MOVQ  AX, sec+0(FP)
    MOVL  DX, nsec+8(FP)
    RET
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Funny story with the &amp;quot;hope that&#39;s enough&amp;quot; comment is that my colleagues at Joyent recently ran into it while investigating a segfaulting golang program on LX. &lt;a href=&#34;https://smartos.org/bugview/OS-5637&#34;&gt;Spoiler alert: it was not enough.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;so-what-s-happening-here&#34;&gt;So what&#39;s happening here?&lt;/h2&gt;

&lt;p&gt;So why is there a difference between Docker for Mac and all the other environments? If it&#39;s something specific to the Mac why don&#39;t we see it without Docker? There&#39;s one more wrench in this story, and that&#39;s Hypervisor Framework and &lt;a href=&#34;https://github.com/mist64/xhyve&#34;&gt;xhyve&lt;/a&gt;. Unlike all the other cases, in Docker for Mac we&#39;re using xhyve, which runs entirely in userspace. Presumably somewhere in the userspace emulation we&#39;re losing a lot of our performance to virtualization.&lt;/p&gt;

&lt;p&gt;This is unfortunately where my enthusiasm for following this to the end starts to flag. That&#39;s probably a disappointing finish, but in fairness I totally warned you! If you want to tackle this further, xhyve is open source and developed on GitHub. The real conclusion is to be careful that you&#39;re benchmarking what you really think you are. Even in cases where you&#39;ve got &amp;quot;pure&amp;quot; CPU benchmarks you can be caught by surprise!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Secrets Management in the Autopilot Pattern</title>
      <link>http://0x74696d.com/posts/secrets-management-in-the-autopilotpattern/</link>
      <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/secrets-management-in-the-autopilotpattern/</guid>
      <description>&lt;p&gt;No matter how we deploy our applications, managing secrets invokes the chicken-and-the-egg problem. Any system for secrets management has to answer the question of how the application authenticates to the system managing the secrets in the first place. Applications built with the Autopilot Pattern can be easily made to leverage secrets management services, but it&#39;s important to understand the security model and threat model of these services.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/secrets-management-in-the-autopilotpattern&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Consul and etcd in the Autopilot Pattern</title>
      <link>http://0x74696d.com/posts/consul-etcd-on-triton/</link>
      <pubDate>Tue, 02 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/consul-etcd-on-triton/</guid>
      <description>&lt;p&gt;Applications developed with the Autopilot Pattern are self-operating and self-configuring but use an external service catalog like Consul or etcd to store and coordinate global state. ContainerPilot sends the service catalog heartbeats to record that an instance of an application is still up and running. Both Consul and etcd have interesting assumptions about their topology that end users deploying on Triton should be aware of.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/consul-etcd-on-triton&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Video - Introduction to the Autopilot Pattern</title>
      <link>http://0x74696d.com/posts/video-intro-autopilot-pattern/</link>
      <pubDate>Wed, 06 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/video-intro-autopilot-pattern/</guid>
      <description>&lt;p&gt;In this video I introduce the Autopilot Pattern with illustrated examples and a walkthrough of our Autopilot Pattern example application and how ContainerPilot makes it easier to containerize applications new and old.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/video-autopilot-pattern&#34;&gt;Check out the full article and the video on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring and scaling with ContainerPilot telemetry</title>
      <link>http://0x74696d.com/posts/monitoring-and-scaling-with-application-telemetry/</link>
      <pubDate>Tue, 12 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/monitoring-and-scaling-with-application-telemetry/</guid>
      <description>&lt;p&gt;Application health checking is a key feature of ContainerPilot (formerly Containerbuddy). The user-defined health check gives us a binary way of determining whether the application is working. If the application is healthy, ContainerPilot sends a heartbeat to the discovery catalog, and if it&#39;s not, other ContainerPilot-enabled applications will stop sending requests to it. But automatic scaling of a service depends on more than just the pass/fail of a health check. Every application has key performance indicators that tell us if the service is nearing overload and should be scaled up or is under-utilized and can be scaled down.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/containerpilot-telemetry&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building sqlite for rust</title>
      <link>http://0x74696d.com/posts/building-sqlite-for-rust/</link>
      <pubDate>Sat, 19 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/building-sqlite-for-rust/</guid>
      <description>

&lt;p&gt;I&#39;ve been playing around with Rust a bit lately and needed to build something with an embedded database, so I reached for SQLite. I need to build SQLite with some specific features enabled and that&#39;s hard to guarantee with system packages cross-platform. This gave me a chance to try out building a Rust project linked with some custom C code. The overall direction I&#39;m taking here is largely coming from &lt;a href=&#34;https://users.rust-lang.org/t/linking-with-custom-c-library/637/4&#34;&gt;this thread&lt;/a&gt; and the Cargo &lt;a href=&#34;http://doc.crates.io/build-script.html#case-study-building-some-native-code&#34;&gt;build script&lt;/a&gt; page. I&#39;m using &lt;a href=&#34;https://github.com/jgallagher/rusqlite&#34;&gt;rusqlite&lt;/a&gt; for the SQLite bindings, so I&#39;ll need to build that as well.&lt;/p&gt;

&lt;h2 id=&#34;project-outline&#34;&gt;Project outline&lt;/h2&gt;

&lt;p&gt;The other nice thing I got to try out here is how to build a sensible project structure for a project that might mix open source and private code. Cargo has some very minimal expectations about the source code directory, but doesn&#39;t dictate much past that. So unlike that &lt;em&gt;other&lt;/em&gt; language &lt;a href=&#34;http://0x74696d.com/posts/go-get-considered-harmful&#34;&gt;(&lt;em&gt;ahem&lt;/em&gt;)&lt;/a&gt; Rust is pretty chill about letting you decide what works for your project and organization. How this reflects on the culture of those two languages is left as an exercise for the reader.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tree /src/tgross/demo
.
├── build.rs
├── Cargo.toml
├── .git/
├── src/
│   └── main.rs
└── target/

$ tree /src/jgallagher/rusqlite
.
├── Cargo.toml
├── .git/
├── libsqlite3-sys/
│   ├── build.rs
│   └── Cargo.toml
│   └── src/
├── src/
│   └── main.rs
└── target/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this directory tree I&#39;ve got my own code namespaced under &lt;code&gt;tgross/&lt;/code&gt; and my library code in the &lt;code&gt;jgallagher/rusqlite&lt;/code&gt; directory. The &lt;code&gt;rusqlite&lt;/code&gt; developers in turn decided to &amp;quot;vendor&amp;quot; their &lt;code&gt;libsqlite3-sys&lt;/code&gt; crate because it&#39;s really just there to create bindings and doesn&#39;t stand on its own. I could just as easily take &lt;code&gt;rusqlite&lt;/code&gt; and vendor it as a Git submodule or subtree at an arbitrary location within my own project&#39;s directory structure. Rather than pretending that our packaging and dependency tree can be described entirely by &lt;code&gt;import&lt;/code&gt;s in our source code, Rust gives us Cargo, and we can give Cargo search paths for libraries.&lt;/p&gt;

&lt;p&gt;This is really important if you want to have repeatable builds and to keep all your hair during development. It means that source control and the on-disk representation of source code is decoupled from the import paths in the source code. If every scrap of code you write and pull from third parties exists in a giant monorepo (like Google does it), maybe you won&#39;t notice. But this means I can start development by pulling from GitHub or &lt;a href=&#34;https://crates.io/&#34;&gt;crates.io&lt;/a&gt;, fork a local copy of a dependency for debugging, or mirror a third-party repository in my CI/CD workflow. (DevOps pro-tip: this means you can still ship software to customers when GitHub is down.) And all of that happens without running around my source tree rewriting imports, or worrying about tree-shaking, or fiddling with environment variables. &lt;em&gt;The Rust developers got this shit right.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ok, rant over. Deep breaths...&lt;/p&gt;

&lt;h2 id=&#34;cargo-toml&#34;&gt;Cargo.toml&lt;/h2&gt;

&lt;p&gt;This is just the &amp;quot;hello world&amp;quot; of &lt;code&gt;rusqlite&lt;/code&gt;, so our demonstration application will be &lt;a href=&#34;https://github.com/jgallagher/rusqlite/blob/master/README.md&#34;&gt;the example code from the rusqlite README&lt;/a&gt;. Here&#39;s our demo app&#39;s Cargo.toml:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;[package]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;demo&amp;quot;&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;version&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;0.1.0&amp;quot;&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;authors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;[&amp;quot;Tim Gross &amp;lt;tim@0x74696d.com&amp;gt;&amp;quot;]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;build&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;build.rs&amp;quot;&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;links&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;libsqlite&amp;quot;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;[dependencies]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;time&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;~0.1.0&amp;quot;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;[dependencies.rusqlite]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;/src/jgallagher/rusqlite&amp;quot;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;[build-dependencies]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;gcc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;quot;0.3&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that I&#39;ve got four different dependencies here and each one is being added in a different way. The most straightforward is the &lt;code&gt;time&lt;/code&gt; crate, which our demonstration app uses to get the current time when inserting a row. We&#39;ve pinned it to a specific version with the &lt;code&gt;~&lt;/code&gt; flag, which means we&#39;ll accept patch version updates but not minor version increases (in the semver sense). When we build, Cargo will fetch this dependency from crates.io, compile it, and then cache the output of that compilation in our target directory for linking down the road.&lt;/p&gt;

&lt;p&gt;We also have a separate &lt;code&gt;[dependencies.rusqlite]&lt;/code&gt; section here, where we&#39;ve specified a path. This path will be an on-disk location where Cargo will try to find the dependency, rather than going out to the Internet for it. This is convenient if I want to work up a patch of &lt;code&gt;rusqlite&lt;/code&gt; or if I&#39;ve got another private project that I want to link in here without fetching it from GitHub (i.e. from the mirror of repos on my CI/CD system). We can also pass feature flags or other compiler options to the dependency when we have it in its own section like this. Another option is to just have a &lt;code&gt;path&lt;/code&gt; field under your Cargo config and have Cargo search there first. But if you have a whole lot of code within those paths (as I do with a fairly flat &lt;code&gt;/src/tgross/&lt;/code&gt; directory), then you&#39;re going to be risking annoying collisions.&lt;/p&gt;

&lt;p&gt;Next we have &lt;code&gt;gcc&lt;/code&gt;, which is marked under &lt;code&gt;[build-dependencies]&lt;/code&gt;. This feature lets you fetch crates for purposes of the build process (or for testing with &lt;code&gt;[dev-dependencies]&lt;/code&gt;), but these crates won&#39;t be linked into your final library or executable binary output. We&#39;re going to use the &lt;a href=&#34;http://alexcrichton.com/gcc-rs/gcc/index.html&#34;&gt;&lt;code&gt;gcc&lt;/code&gt; crate&lt;/a&gt; to assist us with building SQLite.&lt;/p&gt;

&lt;p&gt;Lastly and perhaps less obviously, we have a &lt;code&gt;links&lt;/code&gt; and &lt;code&gt;build&lt;/code&gt; section under &lt;code&gt;[package]&lt;/code&gt;. This is how we&#39;re going to tell Cargo that we have to build and link an external library.&lt;/p&gt;

&lt;h2 id=&#34;build-rs&#34;&gt;build.rs&lt;/h2&gt;

&lt;p&gt;We still need to tell Cargo how to actually build SQLite, and is pretty straightforward with the &lt;code&gt;gcc&lt;/code&gt; crate. In our &lt;a href=&#34;http://doc.crates.io/build-script.html&#34;&gt;&lt;code&gt;build.rs&lt;/code&gt; script&lt;/a&gt; we just need to pass the appropriate arguments to the gcc methods and we&#39;ll get the expected output.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;extern&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;crate&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gcc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;gcc&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;Config&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;define&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;SQLITE_ENABLE_FTS5&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Some&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;define&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;SQLITE_ENABLE_RTREE&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Some&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;define&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;SQLITE_ENABLE_JSON1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Some&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;define&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;SQLITE_ENABLE_DBSTAT_VTAB&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Some&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;define&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;SQLITE_ENABLE_EXPLAIN_COMMENTS&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Some&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;/src/sqlite/src/sqlite3.c&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;libsqlite3.a&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is the equivalent of doing:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span&gt;&lt;/span&gt;gcc -DSQLITE_ENABLE_FTS5&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-DSQLITE_ENABLE_RTREE&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-DSQLITE_ENABLE_JSON1&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-DSQLITE_ENABLE_DBSTAT_VTAB&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-DSQLITE_ENABLE_EXPLAIN_COMMENTS&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-c src/sqlite3.c &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-lpthread -ldl &lt;span class=&#34;se&#34;&gt;\&lt;/span&gt;
	-o libsqlite3.a
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that the linked pthread and ld libs will be provided as part of our standard rust build. Optimization level and whether to include debug symbols will be set according to the Cargo build, but these can be overridden (see the &lt;a href=&#34;http://alexcrichton.com/gcc-rs/gcc/struct.Config.html#method.opt_level&#34;&gt;&lt;code&gt;gcc::Config&lt;/code&gt;&lt;/a&gt; docs).&lt;/p&gt;

&lt;p&gt;Also note here that we&#39;re hard-coding the path to the &lt;a href=&#34;https://www.sqlite.org/amalgamation.html&#34;&gt;SQLite source amalgamation file&lt;/a&gt;, which kinda sucks. The &lt;code&gt;libsqlite3-sys&lt;/code&gt; crate handles this by taking an environment variable, or we could just vendor the &lt;code&gt;sqlite3.c&lt;/code&gt; source and header file alongside our code. If anyone knows a good workaround I&#39;d be interested to hear about it.&lt;/p&gt;

&lt;p&gt;In any case, now we&#39;re ready to build!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span&gt;&lt;/span&gt;$ cargo build
cargo build
    Compiling bitflags v0.1.1
    Compiling winapi v0.2.6
    Compiling libc v0.2.8
    Compiling pkg-config v0.3.8
    Compiling winapi-build v0.1.1
    Compiling gcc v0.3.25
    Compiling kernel32-sys v0.2.1
    Compiling libsqlite3-sys v0.4.0 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file:///src/tgross/demo&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
    Compiling &lt;span class=&#34;nb&#34;&gt;time&lt;/span&gt; v0.1.34
    Compiling demo v0.1.0 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file:///src/tgross/demo&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
    Compiling rusqlite v0.6.0 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file:///src/tgross/demo&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;

$ ldd target/debug/demo
    linux-vdso.so.1 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x00007ffdf7abe000&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
    libdl.so.2 &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x00007f9404...
    libpthread.so.0 &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x...
    libgcc_s.so.1 &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x0000...
    libc.so.6 &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x00007f940385...
    /lib64/ld-linux-x86-64.so.2 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x00007f94045b9000&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
    libm.so.6 &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; /lib/x86_64-linux-gnu/libm.so.6 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;0x00007f940355...

$ ./target/debug/demo
Found person Person &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt; id: &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;, name: &lt;span class=&#34;s2&#34;&gt;&amp;quot;Steven&amp;quot;&lt;/span&gt;,
time_created: Timespec &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt; sec: &lt;span class=&#34;m&#34;&gt;1458435347&lt;/span&gt;, nsec: &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;, data: None &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Production Docker logs on Triton</title>
      <link>http://0x74696d.com/posts/docker-log-drivers/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/docker-log-drivers/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;http://0x74696d.com/posts/docker-logging&#34;&gt;previous post&lt;/a&gt; I talked about an approach I took getting logs out of Docker containers when I first started using Docker way back at the end of 2013. But Docker has done a lot of growing up since then!&lt;/p&gt;

&lt;p&gt;Using &lt;code&gt;docker logs&lt;/code&gt; to get our container logs works in development but in production we need to centralize our logs. Triton has support for the syslog, Graylog, and Fluentd log drivers and we can use them to support production-ready log collection.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/docker-log-drivers&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementing the autopilot pattern</title>
      <link>http://0x74696d.com/posts/applications-on-autopilot/</link>
      <pubDate>Thu, 03 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/applications-on-autopilot/</guid>
      <description>&lt;p&gt;Deploying containerized applications and connecting them together is a challenge because it forces developers to design for operationalization. Autopiloting applications are a powerful design pattern to solving these problems. By pushing the responsibility for understanding startup, shutdown, scaling, and recovery from failure into the application, we can build intelligent architectures that minimize human intervention in operation. But we can&#39;t rewrite all our applications at once, so we need a way to build application containers that can knit together legacy and greenfield applications alike. This project demonstrates the autopilot pattern by applying it to a simple microservices deployment using Nginx and two Node.js applications.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/applications-on-autopilot&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MySQL on Autopilot</title>
      <link>http://0x74696d.com/posts/mysql-on-autopilot/</link>
      <pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/mysql-on-autopilot/</guid>
      <description>&lt;p&gt;DBaaS solutions lock too many doors: we&#39;re locked into a service provider and locked out of configuring it as we need. Fortunately, modern operational patterns are emerging that eliminate the complexity of running even sophisticated applications like databases and free us from the lock-in of *aaS. Now we are free to develop on our laptops and deploy to private data centers, combining the simplicity we thought was only possible with *aaS with the portability of running applications on our own terms. Let&#39;s take a look at how to do that with MySQL.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/dbaas-simplicity-no-lock-in&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog Diet</title>
      <link>http://0x74696d.com/posts/blog-diet/</link>
      <pubDate>Sat, 02 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/blog-diet/</guid>
      <description>

&lt;p&gt;It&#39;s a new year and so why not take the opportunity to revisit my site and make a few improvements? I&#39;ve had a few people mention in the past that they&#39;ve had some readability issues with some of the design choices (particularly on Linux). And as I&#39;ve added more content I&#39;ve found a few little layout quirks that I&#39;m not 100% happy with. Let&#39;s dive right in.&lt;/p&gt;

&lt;h2 id=&#34;the-spec&#34;&gt;The Spec&lt;/h2&gt;

&lt;p&gt;The major design criteria are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Keep page weight low (or lower!).&lt;/li&gt;
&lt;li&gt;Make the page more mobile-friendly.&lt;/li&gt;
&lt;li&gt;Improve the font weights for better accessibility.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;the-audit&#34;&gt;The Audit&lt;/h2&gt;

&lt;p&gt;Partially inspired by Maciej Cegowski&#39;s &lt;a href=&#34;http://idlewords.com/talks/website_obesity.htm&#34;&gt;The Website Obesity Crisis&lt;/a&gt;, I first decided to look into reducing page weight. Fortunately the original design was pretty good in this respect. The home page weighed in at 101KB, most of which were the Google web fonts. My &lt;a href=&#34;http://0x74696d.com/posts/analytics-on-the-cheap&#34;&gt;Analytics on the Cheap&lt;/a&gt; was 184KB, most of the extra load coming from loading the Twitter div. An image-heavy post like &lt;a href=&#34;posts/falling-in-and-out-of-love-with-dynamodb-part-ii/&#34;&gt;Falling In and Out of Love With DynamoDB, Part II&lt;/a&gt; was still only 315KB. So there is some room for improvement here but not a lot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://0x74696d.com/images/20160102/waterfall-before.png&#34; alt=&#34;Homepage waterfall diagram, before&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I took a pass with Google developer tool&#39;s Audit feature and it noted that because I was on an HTTP/1.1 connection that I might want to combine the multiple CSS files. This is optimized for free on HTTP2, but because I&#39;m hosting the blog on Github pages I don&#39;t have control over the server. I also noted that &lt;code&gt;pygments.css&lt;/code&gt; is being loaded even on pages where I&#39;m never going to have code snippets. There are an awful lot of unused CSS rules on the page too, so we can trim some garbage there. The tool also mentioned that there are no far-future cache headers applied to the assets, but again that&#39;s going to be a function of hosting on Github pages (and/or their Fastly CDN configuration). Anything we can do here is going to be a micro-optimization, but what the hell let&#39;s do it anyways.&lt;/p&gt;

&lt;p&gt;The font weights around the menu needed some work, as did the typography on the &lt;a href=&#34;http://0x74696d.com/community.html&#34;&gt;community&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://0x74696d.com/images/20160102/menu-before.png&#34; alt=&#34;Menu, before&#34; /&gt;
&lt;img src=&#34;http://0x74696d.com/images/20160102/community-before.png&#34; alt=&#34;List of talks, before&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-improvements&#34;&gt;The Improvements&lt;/h2&gt;

&lt;p&gt;Although I&#39;m a big fan of &lt;a href=&#34;http://getskeleton.com&#34;&gt;Skeleton&lt;/a&gt; it&#39;s way more than I actually need for this project. There are no forms, buttons, heroes, complex grids, etc. So instead I&#39;m keeping the same basic CSS that I had but just tweaking it to remove cruft.&lt;/p&gt;

&lt;p&gt;I&#39;ve moved the &lt;code&gt;mobile.css&lt;/code&gt; into the &lt;code&gt;base.css&lt;/code&gt; and used a media query. I originally thought that &lt;code&gt;link media=&amp;quot;only screen and (max-device-width: 480px)&amp;quot;&lt;/code&gt; prevented the element from loading, but that turns out not to be true for what should have been obvious reasons -- if we had a &lt;code&gt;max-width&lt;/code&gt; criteria it would force downloading new CSS if someone resized their browser! I checked the page on my older Android tablet and realized that I want to expand the media query to cover smaller tablets as well, so I&#39;ve bumped that up to &lt;code&gt;768px&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I previously failed to notice that the CSS I stole from I-don&#39;t-remember-where caused the menu headers (&amp;quot;posts&amp;quot;, &amp;quot;projects&amp;quot;, &amp;quot;community&amp;quot;, etc.) to be hidden on mobile, so that&#39;s a quick fix. I also noticed that it did that wretched thing where it intentionally broke zooming. Sorry, fixed now!&lt;/p&gt;

&lt;p&gt;The biggest change stylistically was removing the Lato web font that I was loading from Google. Getting rid of that dropped most of the extraneous page weight. I&#39;m not a professional designer and that&#39;s not the focus of the blog, so I don&#39;t really care that much about &amp;quot;pixel perfect&amp;quot; design. Rather the messing around trying to find the perfect font and doubling the size of the page, I&#39;m just going to leave it up to the reader&#39;s machine and browser and &lt;a href=&#34;http://www.smashingmagazine.com/2015/11/using-system-ui-fonts-practical-guide/&#34;&gt;use their system fonts&lt;/a&gt;. The results look good on the five different devices I was able to check. They&#39;re all a bit different of course, but better designers than me picked out those fonts on those platforms so let&#39;s trust them.&lt;/p&gt;

&lt;h2 id=&#34;and-done&#34;&gt;And... Done!&lt;/h2&gt;

&lt;p&gt;I&#39;m pretty happy with the results of just a couple hours of work. There is only one third-party request left on the site (the Twitter bug), and the page is a quarter the size. More importantly, it&#39;s more legible on a wider range of devices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://0x74696d.com/images/20160102/waterfall-after.png&#34; alt=&#34;Homepage waterfall diagram, after&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploy a microservices stack in seconds</title>
      <link>http://0x74696d.com/posts/microservices-stack-in-seconds/</link>
      <pubDate>Fri, 06 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/microservices-stack-in-seconds/</guid>
      <description>&lt;p&gt;Over the last couple weeks I&#39;ve been working on a project at Joyent to demonstrate the components of a container-native microservices architecture. Today I&#39;ve put the pieces together. I&#39;m using &lt;a href=&#34;https://github.com/joyent/containerbuddy&#34;&gt;Containerbuddy&lt;/a&gt; in a stack that includes Nginx, Couchbase, a Node.js application, Cloudflare DNS, and our Triton platform. All the components can be swapped out for your favorite ones just by changing a &lt;code&gt;docker-compose.yml&lt;/code&gt; description.&lt;/p&gt;

&lt;p&gt;And there&#39;s not a scheduler in sight! When you ditch your VMs and deploy on bare metal in an environment where containers have their own NIC(s), you don&#39;t need all that extra overhead. Simple tools will do the job for you.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/how-to-dockerize-a-complete-application&#34;&gt;Check out the full article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic DNS Updates w/ Containerbuddy</title>
      <link>http://0x74696d.com/posts/dynamic-dns-for-docker-w-containerbuddy/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/dynamic-dns-for-docker-w-containerbuddy/</guid>
      <description>&lt;p&gt;In a container-native project, we need to balance the desire for ephemeral infrastructure with the requirement to provide a predictable load-balanced interface with the outside world. By updating DNS records for a domain based on changes in the discovery service, we can make sure our users can reach the load-balancer for our project at all times.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/automatic-dns-updates-with-containerbuddy&#34;&gt;Continue reading this article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Container-Native Architecture [talk]</title>
      <link>http://0x74696d.com/posts/container-native-architecture-talk/</link>
      <pubDate>Fri, 30 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/container-native-architecture-talk/</guid>
      <description>&lt;p&gt;On October 29th I gave a talk on container-native architecture at &lt;a href=&#34;http://dynamicinfradays.org/events/2015-nyc/&#34;&gt;ContainerDays NYC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dynamicinfradays.org/events/2015-nyc/programme.html#architecture&#34;&gt;From the programme:&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Moving your application into a container and deploying it to production is a great first step towards taking advantage of containerization. This gets you past &amp;quot;works on my machine&amp;quot;, and Docker makes this easy. But the real value of containers -- fast immutable deployments, maximizing resource utilization, and bare-metal performance -- comes from an architecture optimized for containers. This is container-native architecture.&lt;/p&gt;

&lt;p&gt;Tim will explore the story of a real-world large scale production microservices deployment of Docker, and the challenges faced in both design and operations of migrating this kind of multi-faceted application to a container-native architecture.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can find my slides &lt;a href=&#34;http://0x74696d.com/talk-containerdays-nyc-2015/#/&#34;&gt;here&lt;/a&gt;. I experimented with &lt;code&gt;reveal.js&lt;/code&gt; for these, so use &lt;code&gt;&amp;lt;space&lt;/code&gt;&amp;gt; to advance and &lt;code&gt;s&lt;/code&gt; to get the speaker&#39;s notes.&lt;/p&gt;

&lt;p&gt;You can watch the talk on &lt;a href=&#34;https://www.youtube.com/watch?v=08BuE6xyRnc&#34;&gt;YouTube&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anti-patterns for service discovery in Docker</title>
      <link>http://0x74696d.com/posts/anti-patterns-for-service-discovery-in-docker/</link>
      <pubDate>Thu, 29 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/anti-patterns-for-service-discovery-in-docker/</guid>
      <description>&lt;p&gt;Common patterns for microservice container deployments carry hidden assumptions that increase operational costs and put availability at risk. Let&#39;s break down the reasons why and how to build discovery for a container-native world.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/container-native-discovery&#34;&gt;Continue reading this article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Nginx upstreams with Containerbuddy</title>
      <link>http://0x74696d.com/posts/dynamic-nginx-upstreams-with-containerbuddy/</link>
      <pubDate>Thu, 29 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>http://0x74696d.com/posts/dynamic-nginx-upstreams-with-containerbuddy/</guid>
      <description>&lt;p&gt;Containerbuddy simplifies service discovery in Docker and provides a workaround for applications not designed from the start for container-native discovery. Here I demonstrate how to make Nginx container-native with Containerbuddy.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.joyent.com/blog/dynamic-nginx-upstreams-with-containerbuddy&#34;&gt;Continue reading this article on the official Joyent blog...&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>